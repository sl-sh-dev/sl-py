Currently houses benchmarks.py and a github/workflows/.yml file
that controls how the benchmarks.py file is run in the cloud.


Current benchmark is modeled after the same eval-pol algorithm
currently in slosh/benches/.

See the workflow yml file for instructions on how to run the 
benchmark.

Benchmarks are also pushed to the sl-sh project on [bencher.dev](https://bencher.dev/console/projects/sl-sh/perf?key=true&reports_per_page=4&branches_per_page=8&testbeds_per_page=8&benchmarks_per_page=8&reports_page=1&branches_page=1&testbeds_page=1&benchmarks_page=1&branches=095c4f41-852a-499a-9e7e-d86a73aab799&testbeds=e3da5f10-961d-4349-88e4-49dc7675e8fd&benchmarks=9d24e358-8b22-4551-9abd-e73b1d9b608b%2Ce53eaaad-0003-4597-9efa-b4b197193d78%2C216e27c0-4c12-43ae-970c-2ec67c8f6e22&measures=919318c9-81dc-4011-913d-af2e1595b605&clear=true&tab=testbeds).
